{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride Attention Matrix:\n",
      "[[  8  44]\n",
      " [ 17 107]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stride_attention_matrix(query, key, stride_tokens):\n",
    "    \"\"\"\n",
    "    Calcola la Stride Attention Matrix tra query e key utilizzando i token specificati come stride.\n",
    "\n",
    "    Argomenti:\n",
    "    query: array numpy con forma (num_queries, embedding_dim)\n",
    "    key: array numpy con forma (num_keys, embedding_dim)\n",
    "    stride_tokens: array di indici dei token con cui calcolare l'attenzione\n",
    "\n",
    "    Ritorna:\n",
    "    stride_attention: array numpy con forma (num_queries, len(stride_tokens))\n",
    "    \"\"\"\n",
    "\n",
    "    num_queries, embedding_dim = query.shape\n",
    "    num_stride_tokens = len(stride_tokens)\n",
    "\n",
    "    # Estrai i token dalla chiave utilizzando gli indici specificati\n",
    "    stride_key = key[stride_tokens]\n",
    "\n",
    "    # Calcolo dei prodotti tra query e stride_key\n",
    "    scores = np.matmul(query, stride_key.transpose())  # (num_queries, len(stride_tokens))\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Esempio di utilizzo\n",
    "query = np.array([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "key = np.array([[0, 1, 2],\n",
    "                [3, 4, 5],\n",
    "                [6, 7, 8],\n",
    "                [9, 10, 11]])\n",
    "\n",
    "# Indici dei token con cui calcolare l'attenzione\n",
    "stride_tokens = [0, 2]\n",
    "\n",
    "result = stride_attention_matrix(query, key, stride_tokens)\n",
    "print(\"Stride Attention Matrix:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride Attention Matrix:\n",
      "[[ 8. 44.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stride_attention_matrix(query, key, stride_tokens):\n",
    "    \"\"\"\n",
    "    Calcola la Stride Attention Matrix tra query e key utilizzando i token specificati come stride.\n",
    "\n",
    "    Argomenti:\n",
    "    query: array numpy con forma (num_queries, embedding_dim)\n",
    "    key: array numpy con forma (num_keys, embedding_dim)\n",
    "    stride_tokens: array di indici dei token con cui calcolare l'attenzione\n",
    "\n",
    "    Ritorna:\n",
    "    stride_attention: array numpy con forma (num_queries, len(stride_tokens))\n",
    "    \"\"\"\n",
    "\n",
    "    num_queries, embedding_dim = query.shape\n",
    "    num_stride_tokens = len(stride_tokens)\n",
    "\n",
    "    stride_attention = np.zeros((num_queries, num_stride_tokens))\n",
    "\n",
    "    for i, token_index in enumerate(stride_tokens):\n",
    "        # Estrai il token corrente dalla chiave\n",
    "        stride_key = key[token_index]\n",
    "\n",
    "        # Calcola l'attenzione solo tra la query corrispondente e il token corrente\n",
    "        scores = np.matmul(query, stride_key)\n",
    "\n",
    "        stride_attention[:, i] = scores.squeeze()\n",
    "\n",
    "    return stride_attention\n",
    "\n",
    "# Esempio di utilizzo\n",
    "query = np.array([[1, 2, 3]])\n",
    "key = np.array([[0, 1, 2],\n",
    "                [3, 4, 5],\n",
    "                [6, 7, 8],\n",
    "                [9, 10, 11]])\n",
    "\n",
    "# Indici dei token con cui calcolare l'attenzione\n",
    "stride_tokens = [0, 2]\n",
    "\n",
    "result = stride_attention_matrix(query, key, stride_tokens)\n",
    "print(\"Stride Attention Matrix:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride Attention Matrix:\n",
      "[[ 17. 107.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def stride_attention_matrix(query, key, query_index, stride_tokens):\n",
    "    \"\"\"\n",
    "    Calcola la Stride Attention Matrix tra una specifica query e le relative key utilizzando i token specificati come stride.\n",
    "\n",
    "    Argomenti:\n",
    "    query: array numpy con forma (num_queries, embedding_dim)\n",
    "    key: array numpy con forma (num_keys, embedding_dim)\n",
    "    query_index: indice della query di interesse\n",
    "    stride_tokens: array di indici dei token con cui calcolare l'attenzione\n",
    "\n",
    "    Ritorna:\n",
    "    stride_attention: array numpy con forma (1, len(stride_tokens))\n",
    "    \"\"\"\n",
    "\n",
    "    embedding_dim = query.shape[1]\n",
    "    num_stride_tokens = len(stride_tokens)\n",
    "\n",
    "    stride_attention = np.zeros((1, num_stride_tokens))\n",
    "\n",
    "    # Estrai la query di interesse\n",
    "    selected_query = query[query_index]\n",
    "\n",
    "    for i, token_index in enumerate(stride_tokens):\n",
    "        # Estrai il token corrente dalla chiave\n",
    "        stride_key = key[token_index]\n",
    "\n",
    "        # Calcola l'attenzione solo tra la query corrispondente e il token corrente\n",
    "        score = np.dot(selected_query, stride_key)\n",
    "\n",
    "        stride_attention[:, i] = score\n",
    "\n",
    "    return stride_attention\n",
    "\n",
    "# Esempio di utilizzo\n",
    "queries = np.array([[1, 2, 3],\n",
    "                    [4, 5, 6],\n",
    "                    [7, 8, 9],\n",
    "                    [10, 11,12]])\n",
    "\n",
    "keys = np.array([[0, 1, 2],\n",
    "                 [3, 4, 5],\n",
    "                 [6, 7, 8],\n",
    "                 [9, 10, 11]])\n",
    "\n",
    "# Indice della query di interesse\n",
    "query_index = 1\n",
    "\n",
    "# Indici dei token con cui calcolare l'attenzione\n",
    "stride_tokens = [0, 2]\n",
    "\n",
    "result = stride_attention_matrix(queries, keys, query_index, stride_tokens)\n",
    "print(\"Stride Attention Matrix:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'cls.seq_relationship.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nessun token trovato per il calcolo dell'attenzione per l'embedding 'capital'.\n",
      "Il token 'France' non è presente nel vocabolario del tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definizione della stride_attention_matrix\n",
    "def stride_attention_matrix(query, key, stride_tokens):\n",
    "    \"\"\"\n",
    "    Calcola la Stride Attention Matrix tra query e key utilizzando i token specificati come stride.\n",
    "\n",
    "    Argomenti:\n",
    "    query: array numpy con forma (num_queries, embedding_dim)\n",
    "    key: array numpy con forma (num_keys, embedding_dim)\n",
    "    stride_tokens: array di indici dei token con cui calcolare l'attenzione\n",
    "\n",
    "    Ritorna:\n",
    "    stride_attention: array numpy con forma (num_queries, len(stride_tokens))\n",
    "    \"\"\"\n",
    "\n",
    "    num_queries, embedding_dim = query.shape\n",
    "    num_stride_tokens = len(stride_tokens)\n",
    "\n",
    "    # Estrai i token dalla chiave utilizzando gli indici specificati\n",
    "    stride_key = key[stride_tokens]\n",
    "\n",
    "    # Calcolo dei prodotti tra query e stride_key\n",
    "    scores = np.matmul(query, stride_key.transpose())  # (num_queries, len(stride_tokens))\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Carica il tokenizer di BERT e il modello preaddestrato\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Testo di input\n",
    "text = \"The capital of France is Paris\"\n",
    "\n",
    "# Tokenizza il testo e converte in tensori\n",
    "tokens = tokenizer(text, return_tensors='pt')\n",
    "attention_mask = tokens['attention_mask']\n",
    "\n",
    "# Applica la maschera di attenzione per mascherare tutto tranne 'capital'\n",
    "capital_index = tokens['input_ids'].tolist()[0].index(tokenizer.convert_tokens_to_ids('capital'))\n",
    "attention_mask[:, :capital_index] = 0\n",
    "attention_mask[:, capital_index+1:] = 0\n",
    "\n",
    "# Esegui l'output del modello\n",
    "outputs = model(input_ids=tokens['input_ids'], attention_mask=attention_mask)\n",
    "\n",
    "# Recupero l'embedding per i token\n",
    "with torch.no_grad():\n",
    "    model_output = model.base_model(input_ids=tokens['input_ids'], attention_mask=attention_mask)\n",
    "embedding_output = model_output.last_hidden_state\n",
    "\n",
    "# Estrai l'embedding per i token del testo\n",
    "embeddings = embedding_output[0]  # prendi solo il primo elemento (non stiamo elaborando un batch)\n",
    "\n",
    "# Definizione degli stride per calcolare l'attenzione tra i token specifici\n",
    "strides = {\n",
    "    'capital': [3, 4],  # 'is' e 'France'\n",
    "    'France': [1],      # 'capital'\n",
    "}\n",
    "\n",
    "for token, stride_tokens in strides.items():\n",
    "    # Verifica se il token è presente nell'input\n",
    "    if token in tokenizer.vocab:\n",
    "        # Indice del token nell'input\n",
    "        token_index = tokens['input_ids'].tolist()[0].index(tokenizer.convert_tokens_to_ids(token))\n",
    "\n",
    "        # Estrai l'embedding del token\n",
    "        query = embeddings[token_index].unsqueeze(0)\n",
    "\n",
    "        # Estrai l'embedding dei token specificati dagli stride\n",
    "        key_indices = [tokens['input_ids'].tolist()[0].index(tokenizer.convert_tokens_to_ids(token)) for token in tokenizer.convert_ids_to_tokens(tokens['input_ids'].tolist()[0]) if tokenizer.convert_tokens_to_ids(token) in stride_tokens]\n",
    "\n",
    "        # Verifica se ci sono indici validi\n",
    "        if len(key_indices) > 0:\n",
    "            key = embeddings[key_indices]\n",
    "\n",
    "            # Calcola la Stride Attention Matrix solo se ci sono token da considerare\n",
    "            stride_attention = stride_attention_matrix(query.numpy(), key.numpy(), list(range(len(stride_tokens))))\n",
    "\n",
    "            print(f\"Attività dell'attenzione per l'embedding '{token}':\")\n",
    "            print(stride_attention)\n",
    "\n",
    "            # Creazione della matrice di attenzione sparsa\n",
    "            attention_matrix = np.zeros((len(stride_tokens), len(stride_tokens)))\n",
    "            for i, token_i in enumerate(stride_tokens):\n",
    "                for j, token_j in enumerate(stride_tokens):\n",
    "                    attention_matrix[i, j] = stride_attention[0, j]\n",
    "\n",
    "            # Plot della matrice di attenzione sparsa\n",
    "            plt.imshow(attention_matrix, cmap='Blues', interpolation='nearest')\n",
    "            plt.title(f'Attention Matrix for token \"{token}\"')\n",
    "            plt.xlabel('Token Indices')\n",
    "            plt.ylabel('Token Indices')\n",
    "            plt.colorbar()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Nessun token trovato per il calcolo dell'attenzione per l'embedding '{token}'.\")\n",
    "    else:\n",
    "        print(f\"Il token '{token}' non è presente nel vocabolario del tokenizer.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stride Attention Matrix:\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Testo di input\n",
    "text = \"The capital of France is Paris\"\n",
    "\n",
    "# Tokenizzazione del testo\n",
    "tokens = text.split()\n",
    "\n",
    "# Definizione degli indici dei token per cui calcolare l'attenzione\n",
    "stride_tokens = [1, 4]  # \"capital\" e \"Paris\"\n",
    "\n",
    "# Creazione della matrice one-hot encoding\n",
    "one_hot_matrix = np.eye(len(tokens))\n",
    "\n",
    "# Rappresentazione one-hot del testo\n",
    "query = one_hot_matrix[[1, 4]]  # Selezioniamo le righe corrispondenti ai token \"capital\" e \"Paris\"\n",
    "\n",
    "# Rappresentazione one-hot dei token della chiave\n",
    "key = one_hot_matrix[:len(tokens)]\n",
    "\n",
    "# Calcolo della Stride Attention Matrix\n",
    "result = stride_attention_matrix(query, key, stride_tokens)\n",
    "\n",
    "print(\"Stride Attention Matrix:\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questo esempio, le parole della frase sono state rappresentate come vettori di embedding e la funzione stride_attention_matrix viene utilizzata per calcolare la matrice di attenzione tra le query (le parole della frase) e la chiave (che sono vettori casuali generati in questo esempio). Si considerano solo il primo e il terzo token della chiave per calcolare l'attenzione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Calcolo della Stride Attention Matrix\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mstride_attention_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Stampare il risultato\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStride Attention Matrix:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m, in \u001b[0;36mstride_attention_matrix\u001b[0;34m(query, key, stride_tokens)\u001b[0m\n\u001b[1;32m     37\u001b[0m num_queries, embedding_dim \u001b[38;5;241m=\u001b[39m query\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     38\u001b[0m stride_key \u001b[38;5;241m=\u001b[39m key[stride_tokens]\n\u001b[0;32m---> 39\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride_key\u001b[49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 2 is different from 10)"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# Definizione degli embedding per le parole della frase\n",
    "word_embeddings = {\n",
    "    'Oggi': [0.1, 0.2, 0.3],\n",
    "    'il': [0.2, 0.3, 0.4],\n",
    "    'sole': [0.3, 0.4, 0.5],\n",
    "    'splende': [0.4, 0.5, 0.6],\n",
    "    'ed': [0.5, 0.6, 0.7],\n",
    "    'è': [0.6, 0.7, 0.8],\n",
    "    'una': [0.7, 0.8, 0.9],\n",
    "    \"c'è\": [0.8, 0.9, 1.0],\n",
    "    'pure': [0.9, 1.0, 1.1],\n",
    "    'caldo': [1.0, 1.1, 1.2]\n",
    "}\n",
    "\n",
    "# Frase di esempio\n",
    "sentence = 'Oggi il sole splende ed è una c\\'è pure caldo'\n",
    "\n",
    "# Convertire la frase in una lista di parole\n",
    "words = sentence.split()\n",
    "\n",
    "# Ottenere gli embedding per ogni parola della frase\n",
    "word_vectors = [word_embeddings[word] for word in words]\n",
    "\n",
    "# Convertire la lista di embedding in un array numpy\n",
    "query = np.array(word_vectors)\n",
    "\n",
    "# Key di esempio (creati casualmente)\n",
    "key = np.random.rand(10, 3)\n",
    "\n",
    "# Indici dei token con cui calcolare l'attenzione\n",
    "stride_tokens = [0, 2]  # Ad esempio, consideriamo solo il primo e il terzo token della key\n",
    "query_token = [3]\n",
    "\n",
    "# Funzione per calcolare la Stride Attention Matrix\n",
    "def stride_attention_matrix(query, key, stride_tokens):\n",
    "    #num_queries, embedding_dim = query.shape\n",
    "    stride_key = key[stride_tokens]\n",
    "    query\n",
    "    scores = np.matmul(query, stride_key.transpose())  \n",
    "    return scores\n",
    "\n",
    "# Calcolo della Stride Attention Matrix\n",
    "result = stride_attention_matrix(query, key, stride_tokens)\n",
    "\n",
    "# Stampare il risultato\n",
    "print(\"Stride Attention Matrix:\")\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvs-classifier",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
